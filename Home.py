import streamlit as st
import pandas as pd
from PIL import Image
from footer import display_footer  # â˜…ã“ã“ã‚’è¿½åŠ â˜…

# ãƒ–ãƒ©ã‚¦ã‚¶ã®ã‚¿ãƒ–åã‚’ã€Œã—ã®ã†ãŸã‚¿ã‚¤ãƒ ã€ã«è¨­å®šã—ã€ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’åºƒã‚ã«è¨­å®š
st.set_page_config(
    page_title="ã—ã®ã†ãŸã‚¿ã‚¤ãƒ ",
    page_icon="ğŸ‘»",
    layout="wide",
)

# --- ã‚«ã‚¹ã‚¿ãƒ CSSã®é©ç”¨ ---
try:
    with open("style.css", encoding="utf-8") as f:
        st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
except FileNotFoundError:
    st.error("ã‚¨ãƒ©ãƒ¼: style.css ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    st.info("`style.css` ãŒã‚¢ãƒ—ãƒªã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
except Exception as e:
    st.error(f"ã‚¨ãƒ©ãƒ¼: style.css ã®èª­ã¿è¾¼ã¿ä¸­ã«å•é¡ŒãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
# --- ã‚«ã‚¹ã‚¿ãƒ CSSã®é©ç”¨ã“ã“ã¾ã§ ---

st.title("ã—ã®ã†ãŸã‚¿ã‚¤ãƒ ğŸ‘»ğŸ«§")

# --- æ¦‚è¦æ¬„ã®è¿½åŠ  ---
st.markdown(
    """
    ã“ã¡ã‚‰ã¯VTuberã€Œ[å¹½éŸ³ã—ã®](https://www.774.ai/talent/shino-kasukane)ã€ã•ã‚“ã®é…ä¿¡ã§æ­Œã‚ã‚ŒãŸæ¥½æ›²ã‚’ã¾ã¨ã‚ãŸéå…¬å¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚
    æ›²åã€ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆã€ãƒ©ã‚¤ãƒ–é…ä¿¡ã‚¿ã‚¤ãƒˆãƒ«ã§æ¤œç´¢ã§ãã¾ã™ã€‚YouTubeãƒªãƒ³ã‚¯ã‹ã‚‰è©²å½“ã®æ­Œå”±ç®‡æ‰€ã«ç›´æ¥é£›ã¹ã¾ã™ã€‚
    """
)
st.markdown("---")
# --- æ¦‚è¦æ¬„ã®è¿½åŠ ã“ã“ã¾ã§ ---

# --- TSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ ---
lives_file_path = "data/M_YT_LIVE.TSV"
songs_file_path = "data/M_YT_LIVE_TIMESTAMP.TSV"


# --- æ™‚é–“æ–‡å­—åˆ—ã‚’ç§’æ•°ã«å¤‰æ›ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def convert_timestamp_to_seconds(timestamp_str):
    if pd.isna(timestamp_str) or not isinstance(timestamp_str, str):
        return None

    parts = list(map(int, timestamp_str.split(":")))

    if len(parts) == 3:
        return parts[0] * 3600 + parts[1] * 60 + parts[2]
    elif len(parts) == 2:
        return parts[0] * 60 + parts[1]
    else:
        return None


# --- ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ ---
df_lives = None
df_songs = None

try:
    df_lives = pd.read_csv(lives_file_path, delimiter="\t")
except FileNotFoundError:
    st.error(f'ã‚¨ãƒ©ãƒ¼: é…ä¿¡æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« "{lives_file_path}" ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')
    st.info(f"`{lives_file_path}` ãŒã‚¢ãƒ—ãƒªã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
except Exception as e:
    st.error(
        f'é…ä¿¡æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« "{lives_file_path}" ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}'
    )

try:
    df_songs = pd.read_csv(songs_file_path, delimiter="\t")
except FileNotFoundError:
    st.error(f'ã‚¨ãƒ©ãƒ¼: æ¥½æ›²æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« "{songs_file_path}" ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚')
    st.info(f"`{songs_file_path}` ãŒã‚¢ãƒ—ãƒªã¨åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
except Exception as e:
    st.error(
        f'æ¥½æ›²æƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ« "{songs_file_path}" ã®èª­ã¿è¾¼ã¿ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}'
    )


# --- ãƒ‡ãƒ¼ã‚¿ã®çµåˆã¨è¡¨ç¤º ---
if df_lives is not None and df_songs is not None:
    # 'ID' (M_YT_LIVE.TSV) ã¨ 'LIVE_ID' (M_YT_LIVE_TIMESTAMP.TSV) ã‚’ã‚­ãƒ¼ã¨ã—ã¦çµåˆ
    df_merged = pd.merge(
        df_songs,
        df_lives[["ID", "é…ä¿¡æ—¥", "ã‚¿ã‚¤ãƒˆãƒ«", "URL"]],
        left_on="LIVE_ID",
        right_on="ID",
        how="left",
        suffixes=("_song", "_live"),
    )

    # çµåˆã«ä½¿ã£ãŸãŒã€é‡è¤‡ã™ã‚‹M_YT_LIVE.TSVå´ã®IDåˆ—ï¼ˆ`ID_live`ï¼‰ã‚’å‰Šé™¤
    df_merged = df_merged.drop(columns=["ID_live"])

    # åˆ—åã‚’åˆ†ã‹ã‚Šã‚„ã™ãå¤‰æ›´
    df_merged = df_merged.rename(
        columns={
            "ID_song": "æ¥½æ›²ID",
            "é…ä¿¡æ—¥": "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_original",
            "ã‚¿ã‚¤ãƒˆãƒ«": "ãƒ©ã‚¤ãƒ–ã‚¿ã‚¤ãƒˆãƒ«",
            "URL": "å…ƒãƒ©ã‚¤ãƒ–URL",
        }
    )
    # è¡¨ç¤ºç”¨ã®ã€Œãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥ã€ã¯ã€å…ƒã®ã€Œãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_originalã€ã‚’ãã®ã¾ã¾ä½¿ã†
    df_merged["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥"] = df_merged["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_original"]

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç§’æ•°ã«å¤‰æ›ã™ã‚‹æ–°ã—ã„åˆ—ã‚’è¿½åŠ 
    df_merged["ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—_ç§’"] = df_merged["ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—"].apply(
        convert_timestamp_to_seconds
    )

    # ã‚½ãƒ¼ãƒˆç”¨ã«æ—¥ä»˜å‹ã«å¤‰æ›ã—ãŸã‚«ãƒ©ãƒ ã‚’ä½œæˆ
    df_merged["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable"] = pd.to_datetime(
        df_merged["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_original"], unit="ms", errors="coerce"
    )

    # UNIXãƒŸãƒªç§’ã§å¤‰æ›ã§ããªã‹ã£ãŸï¼ˆNaTã®ï¼‰è¡Œã«å¯¾ã—ã¦ã€YYYY/MM/DDå½¢å¼ã¨ã—ã¦å†å¤‰æ›ã‚’è©¦ã¿ã‚‹
    mask_nat_sortable = df_merged["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable"].isna()
    if mask_nat_sortable.any():
        try:
            df_merged.loc[mask_nat_sortable, "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable"] = pd.to_datetime(
                df_merged.loc[mask_nat_sortable, "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_original"],
                errors="coerce",
            )
        except Exception as e:
            st.warning(f"ã‚½ãƒ¼ãƒˆç”¨ã®ã€Œãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥ã€å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            st.warning(
                "æ—¥ä»˜ã®å½¢å¼ãŒè¤‡é›‘ãªå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚TSVãƒ•ã‚¡ã‚¤ãƒ«å†…ã®ã€Œé…ä¿¡æ—¥ã€ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )

    # ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥ã®é™é † (æ–°ã—ã„æ—¥ä»˜ãŒä¸Š)ã€ã‹ã¤ãã®ä¸­ã§ LIVE_ID ã®æ˜‡é †ã€ã•ã‚‰ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã®æ˜‡é †ã§ã‚½ãƒ¼ãƒˆ
    df_merged = df_merged.sort_values(
        by=["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable", "LIVE_ID", "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—_ç§’"],
        ascending=[False, True, True],
    ).reset_index(drop=True)

    # YouTubeã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãURLã‚’æ­£ã—ãä½œæˆ
    df_merged["YouTubeã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãURL"] = df_merged.apply(
        lambda row: (
            f"{row['å…ƒãƒ©ã‚¤ãƒ–URL']}&t={int(row['ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—_ç§’'])}s"
            if pd.notna(row["å…ƒãƒ©ã‚¤ãƒ–URL"]) and pd.notna(row["ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—_ç§’"])
            else ""
        ),
        axis=1,
    )

    # --- ä¿®æ­£ã•ã‚ŒãŸæ›²ç›®ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã®é–‹å§‹ ---
    # å„ãƒ©ã‚¤ãƒ–é…ä¿¡å†…ã§æ¥½æ›²ã«é€£ç•ªã‚’æŒ¯ã‚‹
    df_merged["æ›²é †"] = df_merged.groupby("LIVE_ID").cumcount() + 1

    # æ—¥ä»˜ã”ã¨ã«LIVE_IDã«é€£ç•ªã‚’æŒ¯ã‚‹ï¼ˆãƒ©ã‚¤ãƒ–ç•ªå·ï¼‰
    def assign_live_number_per_date(group_df):
        # ãã®æ—¥ä»˜å†…ã®LIVE_IDã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒªã‚¹ãƒˆã‚’å–å¾—ã—ã€å‡ºç¾é †ã«1ã‹ã‚‰ã®ç•ªå·ã‚’æŒ¯ã‚‹
        factor_codes, _ = pd.factorize(group_df["LIVE_ID"])
        group_df["ãƒ©ã‚¤ãƒ–ç•ªå·"] = factor_codes + 1
        # ã“ã®é–¢æ•°ã¯ã€ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®DFã‚’å—ã‘å–ã‚Šã€æ–°ã—ã„åˆ—ã‚’è¿½åŠ ã—ã¦è¿”ã™ã€‚
        # group_keys=Falseã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§ã€å…ƒã®ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼ã¯è‡ªå‹•çš„ã«çµåˆã•ã‚Œã‚‹ãŒã€
        # æ˜ç¤ºçš„ã«å¿…è¦ãªåˆ—ã‚’è¿”ã™ã“ã¨ã§ã€ã‚ˆã‚Šå …ç‰¢ã«ãªã‚‹ã€‚
        return group_df[
            ["LIVE_ID", "ãƒ©ã‚¤ãƒ–ç•ªå·"]
        ]  # LIVE_IDã¨æ–°ã—ãæŒ¯ã‚‰ã‚ŒãŸãƒ©ã‚¤ãƒ–ç•ªå·ã‚’è¿”ã™

    # ãƒ©ã‚¤ãƒ–ç•ªå·ã®è¨ˆç®—ã‚’ä¸€åº¦è¡Œã„ã€çµæœã‚’å…ƒã®DataFrameã«ãƒãƒ¼ã‚¸ã™ã‚‹
    # df_mergedã‹ã‚‰å¿…è¦ãªã‚­ãƒ¼åˆ—ã¨LIVE_IDã‚’å–ã‚Šå‡ºã—ã€ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªçµ„ã¿åˆã‚ã›ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€ãƒ©ã‚¤ãƒ–ç•ªå·ã‚’æŒ¯ã‚‹
    temp_live_numbers = (
        df_merged[["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable", "LIVE_ID"]].drop_duplicates().copy()
    )

    # temp_live_numbersã‚’ã‚½ãƒ¼ãƒˆã—ã¦ã€factorizeã®é †åºã‚’å®‰å®šã•ã›ã‚‹
    temp_live_numbers = temp_live_numbers.sort_values(
        by=["ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable", "LIVE_ID"]
    )

    temp_live_numbers = temp_live_numbers.groupby(
        "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable", group_keys=False
    ).apply(assign_live_number_per_date, include_groups=False)

    # ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ã—ã€ãƒãƒ¼ã‚¸ã«å¿…è¦ãªåˆ—ã®ã¿ã«ã™ã‚‹
    # temp_live_numbers ã¯æ—¢ã« LIVE_ID ã¨ ãƒ©ã‚¤ãƒ–ç•ªå· ã‚’å«ã‚“ã§ã„ã‚‹
    # ãƒãƒ¼ã‚¸ã‚­ãƒ¼ã¨ãªã‚‹ ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable ã¯ã‚°ãƒ«ãƒ¼ãƒ—ã‚­ãƒ¼ã¨ã—ã¦è‡ªå‹•çš„ã«çµåˆã•ã‚Œã‚‹ãŸã‚ã€
    # drop_duplicates() ã§é‡è¤‡ãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ãŸä¸Šã§ã€LIVE_IDã¨ãƒ©ã‚¤ãƒ–ç•ªå·ã ã‘ã‚’ãƒãƒ¼ã‚¸ã™ã‚Œã°ã‚ˆã„

    # df_merged ã«ãƒ©ã‚¤ãƒ–ç•ªå·ã‚’ãƒãƒ¼ã‚¸
    # ã“ã®æ™‚ã€ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable ã¨ LIVE_ID ã‚’çµåˆã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
    df_merged = pd.merge(
        df_merged,
        temp_live_numbers[["LIVE_ID", "ãƒ©ã‚¤ãƒ–ç•ªå·"]],
        on=["LIVE_ID"],
        how="left",
        suffixes=("", "_new"),
    )

    # ã‚‚ã—df_mergedã«ã‚‚ã¨ã‚‚ã¨'ãƒ©ã‚¤ãƒ–ç•ªå·'ãŒã‚ã£ãŸå ´åˆã€ãƒãƒ¼ã‚¸ã§'_new'ãŒä»˜ãã®ã§ã€æ–°ã—ã„æ–¹ã‚’ä½¿ã†
    if "ãƒ©ã‚¤ãƒ–ç•ªå·_new" in df_merged.columns:
        df_merged["ãƒ©ã‚¤ãƒ–ç•ªå·"] = df_merged["ãƒ©ã‚¤ãƒ–ç•ªå·_new"]
        df_merged = df_merged.drop(columns=["ãƒ©ã‚¤ãƒ–ç•ªå·_new"])

    # ãã®æ—¥ä»˜ã«è¤‡æ•°ã®ãƒ©ã‚¤ãƒ–ãŒã‚ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    # df_mergedã«ã¯ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortableãŒæ®‹ã£ã¦ã„ã‚‹ã®ã§ã€ã“ã®ã¾ã¾ä½¿ç”¨ã§ãã‚‹
    live_counts_per_date = df_merged.groupby("ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable")[
        "LIVE_ID"
    ].transform("nunique")

    # æ–°ã—ã„æ›²ç›®å½¢å¼ã‚’ç”Ÿæˆ
    df_merged["æ›²ç›®"] = df_merged.apply(
        lambda row: (
            f"{row['ãƒ©ã‚¤ãƒ–ç•ªå·']}-{row['æ›²é †']}æ›²ç›®"
            if live_counts_per_date.loc[row.name] > 1
            else f"{row['æ›²é †']}æ›²ç›®"
        ),
        axis=1,
    )
    # --- ä¿®æ­£ã•ã‚ŒãŸæ›²ç›®ç”Ÿæˆãƒ­ã‚¸ãƒƒã‚¯ã®çµ‚äº† ---

    st.session_state.df_full = df_merged.copy()

    # --- æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã¨ãƒœã‚¿ãƒ³ã€ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®è¿½åŠ  ---
    # session_state ã®åˆæœŸåŒ–
    if "search_query" not in st.session_state:
        st.session_state.search_query = ""
    if "filtered_df" not in st.session_state:
        st.session_state.filtered_df = st.session_state.df_full.copy()
    if "include_live_title" not in st.session_state:
        st.session_state.include_live_title = True
    if "display_limit" not in st.session_state:
        st.session_state.display_limit = 25  # åˆæœŸè¡¨ç¤ºä»¶æ•°
    # search_query_prev ã¨ include_live_title_prev ã®åˆæœŸåŒ–ã‚’è¿½åŠ 
    if "search_query_prev" not in st.session_state:
        st.session_state.search_query_prev = st.session_state.search_query
    if "include_live_title_prev" not in st.session_state:
        st.session_state.include_live_title_prev = st.session_state.include_live_title

    # â˜…â˜…â˜… ä¸å…·åˆä¿®æ­£ç‚¹ â˜…â˜…â˜…
    # valueå¼•æ•°ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ãŒæ„å›³ã›ãšãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚
    # keyå¼•æ•°ã«ã‚ˆã‚Šã€ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆã®çŠ¶æ…‹ã¯ä¿æŒã•ã‚Œã‚‹ãŸã‚ã€å…¥åŠ›ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¯æ¶ˆãˆã¾ã›ã‚“ã€‚
    current_input = st.text_input(
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆæ›²åã€ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆï¼‰",
        key="search_input_box",
        placeholder="ã“ã“ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›",
    )

    # â˜…â˜…â˜… ä¸å…·åˆä¿®æ­£ç‚¹ â˜…â˜…â˜…
    # ã“ã¡ã‚‰ã‚‚åŒæ§˜ã«valueå¼•æ•°ã‚’å‰Šé™¤ã—ã¾ã™ã€‚
    current_checkbox_value = st.checkbox(
        "æ¤œç´¢å¯¾è±¡ã«ãƒ©ã‚¤ãƒ–é…ä¿¡ã‚¿ã‚¤ãƒˆãƒ«ã‚’å«ã‚ã‚‹",
        key="include_live_title_checkbox",
    )

    search_button = st.button("æ¤œç´¢")

    # æ¤œç´¢ãƒ­ã‚¸ãƒƒã‚¯ã®èª¿æ•´
    # ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸå ´åˆã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’å†å®Ÿè¡Œ
    if search_button:

        st.session_state.search_query = current_input
        st.session_state.include_live_title = current_checkbox_value
        st.session_state.display_limit = 25  # æ¤œç´¢æ¡ä»¶ãŒå¤‰ã‚ã£ãŸã‚‰ãƒªã‚»ãƒƒãƒˆ

        if st.session_state.search_query:
            filter_condition = st.session_state.df_full["æ›²å"].astype(
                str
            ).str.contains(
                st.session_state.search_query, case=False, na=False
            ) | st.session_state.df_full[
                "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ"
            ].astype(
                str
            ).str.contains(
                st.session_state.search_query, case=False, na=False
            )

            if st.session_state.include_live_title:
                filter_condition = filter_condition | st.session_state.df_full[
                    "ãƒ©ã‚¤ãƒ–ã‚¿ã‚¤ãƒˆãƒ«"
                ].astype(str).str.contains(
                    st.session_state.search_query, case=False, na=False
                )

            st.session_state.filtered_df = st.session_state.df_full[
                filter_condition
            ].copy()
            st.write(
                f"ã€Œ{st.session_state.search_query}ã€ã§æ¤œç´¢ã—ãŸçµæœ: {len(st.session_state.filtered_df)}ä»¶"
            )
        else:
            st.session_state.filtered_df = st.session_state.df_full.copy()
            st.write("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…¨ä»¶è¡¨ç¤ºã—ã¾ã™ã€‚")
    # åˆæœŸè¡¨ç¤ºæ™‚ã‚„ã€æ¤œç´¢ãƒœã‚¿ãƒ³ä»¥å¤–ã§ä½•ã‚‚å¤‰ã‚ã£ã¦ã„ãªã„å ´åˆ (ã‹ã¤ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒç©ºã§ãªã„å ´åˆã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç¶­æŒ)
    elif (
        st.session_state.search_query
    ):  # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒç©ºã§ãªã‘ã‚Œã°ä»¥å‰ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœã‚’ç¶­æŒ
        st.write(
            f"ã€Œ{st.session_state.search_query}ã€ã§æ¤œç´¢ã—ãŸçµæœ: {len(st.session_state.filtered_df)}ä»¶"
        )
    else:  # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒç©ºã®å ´åˆã¯å…¨ä»¶è¡¨ç¤º
        st.session_state.filtered_df = st.session_state.df_full.copy()
        st.write("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚å…¨ä»¶è¡¨ç¤ºã—ã¾ã™ã€‚")

    # æ¤œç´¢ã‚¯ã‚¨ãƒªã¨ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®ä»¥å‰ã®çŠ¶æ…‹ã‚’ä¿å­˜ï¼ˆæ¬¡å›ã®å…¥åŠ›å¤‰æ›´æ¤œçŸ¥ç”¨ï¼‰
    st.session_state.search_query_prev = current_input
    st.session_state.include_live_title_prev = current_checkbox_value

    # ã“ã“ã‹ã‚‰æ®µéšçš„è¡¨ç¤ºã®å‡¦ç†
    df_to_show = st.session_state.filtered_df.copy()

    # YouTubeãƒªãƒ³ã‚¯ã‚’HTMLå½¢å¼ã§ç›´æ¥åŸ‹ã‚è¾¼ã‚€ãŸã‚ã«å¤‰æ›
    df_to_show["YouTubeãƒªãƒ³ã‚¯"] = df_to_show.apply(
        lambda row: f'<a href="{row["YouTubeã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãURL"]}" target="_blank">YouTubeã¸ğŸ‘»</a>',
        axis=1,
    )

    # --- ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ ---
    # â˜… 'ãƒ©ã‚¤ãƒ–ç•ªå·'ã¨'æ›²é †'ã¯ã€'æ›²ç›®'ç”Ÿæˆå¾Œã«å‰Šé™¤ã—ã¦ã‚‚è‰¯ã„ã§ã™ãŒã€
    # â˜… ã“ã“ã§ã¯æ®‹ã™ã¾ã¾ã§ã€æœ€çµ‚è¡¨ç¤ºåˆ—ã‹ã‚‰é™¤å¤–ã—ã¦ã„ã¾ã™ã€‚
    df_to_show = df_to_show.drop(
        columns=[
            "YouTubeã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãURL",  # HTMLãƒªãƒ³ã‚¯ç”Ÿæˆå¾Œã«å‰Šé™¤
            "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_original",
            "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥_sortable",
            "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—_ç§’",
            "LIVE_ID",
            "æ¥½æ›²ID",
            "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—",
            "ãƒ©ã‚¤ãƒ–ã‚¿ã‚¤ãƒˆãƒ«",
            "å…ƒãƒ©ã‚¤ãƒ–URL",
        ],
        errors="ignore",
    )

    # ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆåˆ—ã«ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã‚’é©ç”¨ (style.cssã« .artist-cell ãŒå®šç¾©ã•ã‚Œã¦ã„ã‚Œã°æ©Ÿèƒ½ã—ã¾ã™)
    df_to_show["ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ"] = (
        df_to_show["ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ"]
        .astype(str)
        .apply(lambda x: f'<div class="artist-cell">{x}</div>')
    )

    # è¡¨ç¤ºã™ã‚‹åˆ—ã®é †åºã‚’å†èª¿æ•´
    final_display_columns = [
        "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥",
        "æ›²ç›®",
        "æ›²å",
        "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ",
        "YouTubeãƒªãƒ³ã‚¯",
    ]
    # å®Ÿéš›ã«DataFrameã«å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é¸æŠã—ã¦è¡¨ç¤º
    final_display_columns = [
        col for col in final_display_columns if col in df_to_show.columns
    ]

    # è¡¨ç¤ºä»¶æ•°ã‚’åˆ¶é™
    df_limited_display = df_to_show[final_display_columns].head(
        st.session_state.display_limit
    )

    # DataFrameã‚’HTMLã¨ã—ã¦ç”Ÿæˆ
    html_table = df_limited_display.to_html(
        escape=False, index=False, justify="left", classes="dataframe"
    )

    # ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç½®ãæ›ãˆè¾æ›¸
    custom_headers = {
        "ãƒ©ã‚¤ãƒ–é…ä¿¡æ—¥": "é…ä¿¡æ—¥",
        "æ›²ç›®": "No.",
        "æ›²å": "æ›²å",
        "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ": "ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ",
        "YouTubeãƒªãƒ³ã‚¯": "ãƒªãƒ³ã‚¯",
    }

    # HTMLæ–‡å­—åˆ—å†…ã§å„ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç½®ãæ›ãˆã‚‹
    for original, custom in custom_headers.items():
        html_table = html_table.replace(f"<th>{original}</th>", f"<th>{custom}</th>")

    # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¯èƒ½ãªdivã§å›²ã‚€
    scrollable_html = f"""
    <div style="overflow-x: auto; max-width: 100%;">
        {html_table}
    </div>
    """
    # ç”Ÿæˆã—ãŸHTMLã‚’Streamlitã§è¡¨ç¤º
    st.write(scrollable_html, unsafe_allow_html=True)

    # ã€Œã‚‚ã£ã¨è¦‹ã‚‹ã€ãƒœã‚¿ãƒ³
    if st.session_state.display_limit < len(st.session_state.filtered_df):
        if st.button(
            f"ã•ã‚‰ã«25ä»¶è¡¨ç¤ºï¼ˆç¾åœ¨ã®è¡¨ç¤º: {min(st.session_state.display_limit, len(st.session_state.filtered_df))}/{len(st.session_state.filtered_df)}ä»¶ï¼‰"
        ):
            st.session_state.display_limit += 25
            st.rerun()
    else:
        st.info(f"å…¨ã¦ã®{len(st.session_state.filtered_df)}ä»¶ãŒè¡¨ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚")

else:
    st.warning(
        "å¿…è¦ãªTSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã™ã¹ã¦èª­ã¿è¾¼ã‚ãªã‹ã£ãŸãŸã‚ã€çµåˆãƒ‡ãƒ¼ã‚¿ã¯è¡¨ç¤ºã§ãã¾ã›ã‚“ã€‚"
    )

# ãƒ•ãƒƒã‚¿ãƒ¼ã®è¡¨ç¤º
display_footer()  # â˜…ã“ã“ã‚’å‘¼ã³å‡ºã™â˜…
